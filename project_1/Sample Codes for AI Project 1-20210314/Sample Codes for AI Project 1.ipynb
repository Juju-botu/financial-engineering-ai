{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction to Financial Engineering [IE471]\n",
    "##  Hands-on Practice on Financial AI Session #1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Import Packages\n",
    "#### Please import pandas_datareader package using 'pip install pandas_datareader'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pandas_datareader.data as pdr\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import datetime\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.autograd import Variable \n",
    "\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.preprocessing import MinMaxScaler"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Loading the Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start = (2000, 1, 1)  # 2000-01-01 \n",
    "start_time = datetime.datetime(*start)  \n",
    "end = (2020, 12, 31) # 2020-12-31\n",
    "end_time = datetime.datetime(*end) \n",
    "\n",
    "# Loading Samsung Electronic Co,. Ltd. (KS: 005930) Data from Yahoo Finance\n",
    "df = pdr.DataReader('005930.KS', 'yahoo', start_time, end_time)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "High : HIgh Price, The highest price at which a stock traded during that particular day\n",
    "\n",
    "Low : Low Price, The lowest price at which a stock traded during that particular day\n",
    "\n",
    "Open : Open Price, The stock price at which opens at the start of market\n",
    "\n",
    "Close : Close Price, The stock closing at the end of the market hours\n",
    "\n",
    "Volume : Trading volume during that particular day\n",
    "\n",
    "Adj Close : A stock's closing price to reflect that stock's value after accounting for any corporate actions (Reference: https://help.yahoo.com/kb/SLN28256.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df # Using high price, low price, Open price, Close price, Volume, and Adjusted Close Price to predict adjusted close price\n",
    "y = df.iloc[:, 5:6] #Predicting adjusted close price"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Scaling and Converting Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that there is a big difference between volume and other values. In this case, LSTM models need to recognize the difference between these two scales: volume and the rest values, which can be a burden to train data. Therefore, we reduce by scaling all variables from 0 to 1 to reduce the burden on the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MMS = MinMaxScaler()\n",
    "\n",
    "\n",
    "X_data = MMS.fit_transform(X)\n",
    "y_data = MMS.fit_transform(y) \n",
    "\n",
    "# Input Variables (6 variables)\n",
    "X_train = X_data[:4781, :] #We trained 2000 - 2018 data including high price, low price, open price, close price, volume, and adj close data\n",
    "X_test = X_data[4781:, :] #To verify trained model, we used 2019 - 2020 data as the test set\n",
    "\n",
    "# Output Variables (adjusted close price)\n",
    "y_train = y_data[:4781, :]\n",
    "y_test = y_data[4781:, :] \n",
    "\n",
    "print(\"Training Set Shape\", X_train.shape, y_train.shape)\n",
    "print(\"Testing Set Shape\", X_test.shape, y_test.shape) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_tensors = Variable(torch.Tensor(X_train))\n",
    "X_test_tensors = Variable(torch.Tensor(X_test))\n",
    "\n",
    "y_train_tensors = Variable(torch.Tensor(y_train))\n",
    "y_test_tensors = Variable(torch.Tensor(y_test))\n",
    "\n",
    "X_train_tensors_final = torch.reshape(X_train_tensors,   (X_train_tensors.shape[0], 1, X_train_tensors.shape[1]))\n",
    "X_test_tensors_final = torch.reshape(X_test_tensors,  (X_test_tensors.shape[0], 1, X_test_tensors.shape[1])) \n",
    "\n",
    "print(\"Training Shape\", X_train_tensors_final.shape, y_train_tensors.shape)\n",
    "print(\"Testing Shape\", X_test_tensors_final.shape, y_test_tensors.shape) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cpu')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Building LSTM Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTM_model(nn.Module):\n",
    "  def __init__(self, num_classes, input_size, hidden_size, num_layers, seq_length):\n",
    "    super(LSTM_model, self).__init__()\n",
    "    self.num_classes = num_classes #number of classes\n",
    "    self.num_layers = num_layers #number of layers\n",
    "    self.input_size = input_size #input size\n",
    "    self.hidden_size = hidden_size #hidden state\n",
    "    self.seq_length = seq_length #sequence length\n",
    " \n",
    "    self.lstm = nn.LSTM(input_size=input_size, hidden_size=hidden_size, num_layers=num_layers, batch_first=True) #LSTM layer\n",
    "    self.fc = nn.Linear(hidden_size, num_classes) #fully connected last layer\n",
    "\n",
    "  def forward(self,x):\n",
    "    h0 = Variable(torch.zeros(self.num_layers, x.size(0), self.hidden_size)).to(device) #hidden state\n",
    "    c0 = Variable(torch.zeros(self.num_layers, x.size(0), self.hidden_size)).to(device) #internal state   \n",
    "    # Propagate input through LSTM\n",
    "\n",
    "    out, _ = self.lstm(x, (h0, c0))  # out: tensor of shape (batch_size, seq_length, hidden_size)\n",
    "        \n",
    "    # Decode the hidden state of the last time step\n",
    "    out = self.fc(out[:, -1, :])\n",
    "    return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Setting Hyperparameters and Training Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs = 5000\n",
    "learning_rate = 0.001\n",
    "\n",
    "input_size = 6 # number of features\n",
    "hidden_size = 2 # number of features in hidden state\n",
    "num_layers = 1 # number of stacked LSTM layers\n",
    "seq_length = 5 # To predict adj close price of sixth day using data of 5 previous days\n",
    "\n",
    "num_classes = 1 #number of output classes \n",
    "LSTM_1 = LSTM_model(num_classes, input_size, hidden_size, num_layers, X_train_tensors_final.shape[1]).to(device)\n",
    "\n",
    "loss_function = torch.nn.MSELoss()    # mean-squared error for regression\n",
    "optimizer = torch.optim.Adam(LSTM_1.parameters(), lr=learning_rate)  # adam optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for epoch in range(num_epochs+1):\n",
    "    outputs = LSTM_1.forward(X_train_tensors_final.to(device)) #forward pass\n",
    "    optimizer.zero_grad() #caluclate the gradient, manually setting to 0\n",
    "\n",
    "    # obtain the loss function\n",
    "    loss = loss_function(outputs, y_train_tensors.to(device))\n",
    "\n",
    "    loss.backward() #calculates the loss of the loss function\n",
    "\n",
    "    optimizer.step() #improve from loss, i.e backprop\n",
    "    \n",
    "    if epoch % 100 == 0:\n",
    "        print(\"Epoch: %d, Loss: %1.4f\" % (epoch, loss.item())) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.rcParams[\"figure.figsize\"] = (32,18)\n",
    "plt.rcParams[\"figure.dpi\"] = 300\n",
    "plt.rcParams['axes.grid'] = True \n",
    "plt.rcParams.update({'font.size': 20})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6. Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_data = MMS.fit_transform(X)\n",
    "y_data = MMS.fit_transform(y) \n",
    "\n",
    "df_X = Variable(torch.Tensor(X_data)) #converting to Tensors\n",
    "df_y = Variable(torch.Tensor(y_data))\n",
    "#reshaping the dataset\n",
    "df_X = torch.reshape(df_X, (df_X.shape[0], 1, df_X.shape[1]))\n",
    "train_predict = LSTM_1(df_X.to(device))#forward pass\n",
    "data_predict = train_predict.data.detach().cpu().numpy() #numpy conversion\n",
    "dataY_plot = df_y.data.numpy()\n",
    "\n",
    "data_predict = MMS.inverse_transform(data_predict) #reverse transformation\n",
    "dataY_plot = MMS.inverse_transform(dataY_plot)\n",
    "\n",
    "plt.xlim(datetime.datetime(2000, 1, 1), datetime.datetime(2020, 12, 31))\n",
    "test_line = datetime.datetime(2019, 1, 1)\n",
    "plt.axvline(x=test_line, c='r', linestyle='--') #size of the training set\n",
    "\n",
    "plt.plot(df.index.to_pydatetime(), dataY_plot, label='Actual Data') # plot for actual data\n",
    "plt.plot(df.index.to_pydatetime(), data_predict, label='Predicted Data') # plot for predicted data\n",
    "plt.title('Time-Series Prediction (All Period)')\n",
    "plt.legend()\n",
    "plt.show() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MSE = float(sum((dataY_plot - data_predict)*(dataY_plot - data_predict))/len(data_predict))\n",
    "print('Mean Squared Error:', MSE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.xlim(datetime.datetime(2000, 1, 1), datetime.datetime(2018, 12, 31))\n",
    "plt.plot(df.index.to_pydatetime(), dataY_plot, label='Actual Data') # plot for actual data\n",
    "plt.plot(df.index.to_pydatetime(), data_predict, label='Predicted Data') # plot for predicted data\n",
    "plt.title('Time-Series Prediction (Training Period)')\n",
    "plt.legend()\n",
    "plt.show() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.xlim(datetime.datetime(2019, 1, 1), datetime.datetime(2020, 12, 31))\n",
    "plt.plot(df.index.to_pydatetime(), dataY_plot, label='Actual Data') # plot for actual data\n",
    "plt.plot(df.index.to_pydatetime(), data_predict, label='Predicted Data') # plot for predicted data\n",
    "plt.title('Time-Series Prediction (Testing Period)')\n",
    "plt.legend()\n",
    "plt.show() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
